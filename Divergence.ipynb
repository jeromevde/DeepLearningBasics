{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kullback Leilber divergence\n",
    "based on [this](https://machinelearningmastery.com/divergence-between-probability-distributions/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```nn.KLDivLoss``` is the pytorch implementation of the *Kullback-Leibler divergence $D_{KL}$* loss measure. Kullback-Leibler divergence is a measure of how one probability distribution is different from a second, reference probability distribution  \n",
    "Notation: $KL(P||Q)$ represents the Kullback Leilber divergence from Q (model) to P (true distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-ebffec50342f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-ebffec50342f>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    ```nn.KLDivLoss``` is the pytorch implementation of the *Kullback-Leibler divergence $D_{KL}$* loss measure. Kullback-Leibler divergence is a measure of how one probability distribution is different from a second, reference probability distribution\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Kullback Leibler divergence\n",
    "```nn.KLDivLoss``` is the pytorch implementation of the *Kullback-Leibler divergence $D_{KL}$* loss measure. Kullback-Leibler divergence is a measure of how one probability distribution is different from a second, reference probability distribution  \n",
    "Notation: $KL(P||Q)$ represents the Kullback Leilber divergence from Q (model) to P (true distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = ['red', 'green', 'blue']\n",
    "p = [0.10, 0.40, 0.50]\n",
    "q = [0.80, 0.15, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P=1.000 Q=1.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot of distributions\n",
    "from matplotlib import pyplot\n",
    "# define distributions\n",
    "events = ['red', 'green', 'blue']\n",
    "p = [0.10, 0.40, 0.50]\n",
    "q = [0.80, 0.15, 0.05]\n",
    "print('P=%.3f Q=%.3f' % (sum(p), sum(q)))\n",
    "# plot first distribution\n",
    "pyplot.subplot(2,1,1)\n",
    "pyplot.bar(events, p)\n",
    "# plot second distribution\n",
    "pyplot.subplot(2,1,2)\n",
    "pyplot.bar(events, q)\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the kl divergence\n",
    "import math\n",
    "def kl_divergence(p, q):\n",
    "\treturn sum(p[i] * math.log2(p[i]/q[i]) for i in range(len(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(P || Q): 1.927 bits\n",
      "KL(Q || P): 2.022 bits\n"
     ]
    }
   ],
   "source": [
    "# calculate (P || Q)\n",
    "kl_pq = kl_divergence(p, q)\n",
    "print('KL(P || Q): %.3f bits' % kl_pq)\n",
    "# calculate (Q || P)\n",
    "kl_qp = kl_divergence(q, p)\n",
    "print('KL(Q || P): %.3f bits' % kl_qp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jensen Shannon divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jensen-Shannon divergence, or JS divergence for short, is another way to quantify the difference (or similarity) between two probability distributions.\n",
    "\n",
    "It uses the KL divergence to calculate a normalized score that is symmetrical. This means that the divergence of P from Q is the same as Q from P, or stated formally:\n",
    "\n",
    "JS(P || Q) == JS(Q || P)\n",
    "The JS divergence can be calculated as follows:\n",
    "\n",
    "JS(P || Q) = 1/2 * KL(P || M) + 1/2 * KL(Q || M)\n",
    "Where M is calculated as:\n",
    "\n",
    "M = 1/2 * (P + Q)\n",
    "And KL() is calculated as the KL divergence described in the previous section.\n",
    "\n",
    "It is more useful as a measure as it provides a smoothed and normalized version of KL divergence, with scores between 0 (identical) and 1 (maximally different), when using the base-2 logarithm.\n",
    "\n",
    "The square root of the score gives a quantity referred to as the Jensen-Shannon distance, or JS distance for short.\n",
    "\n",
    "We can make the JS divergence concrete with a worked example.\n",
    "\n",
    "First, we can define a function to calculate the JS divergence that uses the kl_divergence() function prepared in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JS(P || Q) divergence: 0.420 bits\n",
      "JS(P || Q) distance: 0.648\n",
      "JS(Q || P) divergence: 0.420 bits\n",
      "JS(Q || P) distance: 0.648\n"
     ]
    }
   ],
   "source": [
    "# example of calculating the js divergence between two mass functions\n",
    "from math import log2\n",
    "from math import sqrt\n",
    "from numpy import asarray\n",
    "\n",
    "# calculate the kl divergence\n",
    "def kl_divergence(p, q):\n",
    "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    "\n",
    "# calculate the js divergence\n",
    "def js_divergence(p, q):\n",
    "\tm = 0.5 * (p + q)\n",
    "\treturn 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n",
    "\n",
    "# define distributions\n",
    "p = asarray([0.10, 0.40, 0.50])\n",
    "q = asarray([0.80, 0.15, 0.05])\n",
    "# calculate JS(P || Q)\n",
    "js_pq = js_divergence(p, q)\n",
    "print('JS(P || Q) divergence: %.3f bits' % js_pq)\n",
    "print('JS(P || Q) distance: %.3f' % sqrt(js_pq))\n",
    "# calculate JS(Q || P)\n",
    "js_qp = js_divergence(q, p)\n",
    "print('JS(Q || P) divergence: %.3f bits' % js_qp)\n",
    "print('JS(Q || P) distance: %.3f' % sqrt(js_qp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:attention]",
   "language": "python",
   "name": "conda-env-attention-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
